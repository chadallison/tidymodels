---
title: "classification with tidymodels"
author: "chad allison | 12 january 2023"
output: github_document
---

___

### loading required libraries and setting options

```{r message = F, warning = F}
library(tidyverse) # essential functions
library(tidymodels) # essential for tidy modeling
library(visdat) # visualising data class structure
library(skimr) # data skimming
library(GGally) # pairwise plots
library(ggmap) # geographical visualisation
library(ranger) # random forest
library(keras) # neural network
library(vip) # variable importance

knitr::opts_chunk$set(message = F, warning = F)
options(scipen = 999)
theme_set(theme_minimal())

custom_red = "#FFCFCF"
custom_green = "#B7D1B3"
tictoc::tic()
```

___

### importing data

```{r}
link = "https://raw.githubusercontent.com/kirenz/datasets/master/housing_unclean.csv"
housing_df = read_csv(link, col_types = cols())
rm(link)

head(housing_df)
```

___

### cleaning `housing_median_age` and `median_house_value`

```{r}
housing_df = housing_df |>
  mutate(housing_median_age = str_remove_all(housing_median_age, "[years]"),
         median_house_value = str_remove_all(median_house_value, "[$]"))

housing_df |>
  select(housing_median_age, median_house_value) |>
  head()
```

___

### viewing data structure

```{r}
glimpse(housing_df)
```

___

### visualising data structure

```{r}
vis_dat(housing_df) +
  scale_fill_manual(values = c("#8BB895", "#B494C6"))
```

___

### exploring `ocean_proximity` variable

```{r}
housing_df |>
  count(ocean_proximity,
        sort = T)
```

___

### reformatting variables

```{r}
housing_df = housing_df |>
  mutate(housing_median_age = as.numeric(housing_median_age),
         median_house_value = as.numeric(median_house_value),
         across(where(is.character), as.factor))

vis_dat(housing_df) +
  scale_fill_manual(values = c("#8BB895", "#B494C6"))
```

___

### checking missing data

```{r}
housing_df |>
  vis_miss(sort_miss = T)
```

___

### seeing exactly how many `NA` values we have

```{r}
colSums(is.na(housing_df))
```

___

### creating new variables

```{r}
housing_df = housing_df |>
  mutate(rooms_per_household = round(total_rooms / households, 4),
         bedrooms_per_room = round(total_bedrooms / total_rooms, 4),
         population_per_household = round(population / households, 4))

housing_df |>
  select(rooms_per_household, bedrooms_per_room, population_per_household) |>
  head()
```

___

### creating dependent variable and dropping original numeric variable

```{r}
housing_df = housing_df |>
  mutate(price_category = case_when(median_house_value < 150000 ~ "below",
                                    median_house_value >= 150000 ~ "above"),
         price_category = as.factor(price_category)) |>
  select(-median_house_value)

housing_df |>
  count(price_category) |>
  mutate(prop = round(n / sum(n), 3))
```

___

### data overview with `skimr`

```{r}
skim(housing_df)
```

___

### data overview with pariwise plots from `GGally`

```{r message = F, warning = F}
housing_df |>
  sample_n(100) |> # sampling for script run time
  select(housing_median_age, median_income, rooms_per_household,
         ocean_proximity, price_category) |>
  ggpairs()
```

___

### data splitting

```{r}
set.seed(123)
data_split = initial_split(housing_df, prop = 0.75, strata = price_category)
train_data = training(data_split)
test_data = testing(data_split)

raw_counts = housing_df |>
  count(price_category) |>
  mutate(prop = round(n / sum(n), 4),
         set = "raw")

train_counts = train_data |>
  count(price_category) |>
  mutate(prop = round(n / sum(n), 4),
         set = "train")

test_counts = test_data |>
  count(price_category) |>
  mutate(prop = round(n / sum(n), 4),
         set = "test")

rbind(raw_counts, train_counts, test_counts) |>
  ggplot(aes(price_category, prop)) +
  geom_col(aes(fill = set), position = "dodge", alpha = 0.75) +
  scale_fill_manual(values = c("#828BA8", "#99BF9E", "#B293BD")) +
  theme_classic() +
  labs(title = "`price_category` equally distributed among full, training, and testing data") +
  theme(plot.title = element_text(hjust = 0.5))

rm(raw_counts, train_counts, test_counts)
```

___

### geographical overview

```{r message = F, warning = F}
data_explore = train_data # so we don't alter our data

qmplot(x = longitude, y = latitude, data = data_explore,
       geom = "point", col = price_category, size = population, alpha = 0.25) +
  scale_alpha(guide = "none") +
  scale_color_manual(values = c("indianred3", "lightsteelblue3"))
```

___

### exploring numeric variables

```{r}
data_explore |>
  ggplot(aes(price_category, median_income)) +
  geom_boxplot(aes(fill = price_category),
               outlier.alpha = 0.25, alpha = 0.5) +
  scale_fill_manual(values = c("indianred3", "springgreen4"))
```

___

### creating function to print boxplot

```{r}
print_boxplot = function(.y_var) {
  y_var = sym(.y_var)
  
  data_explore |>
    ggplot(aes(price_category, {{y_var}})) +
    geom_boxplot(aes(fill = price_category), outlier.alpha = 0.25, alpha = 0.5) +
    scale_fill_manual(values = c("indianred3", "springgreen4"))
}
```

___

### obtaining numeric y-variables

```{r}
y_var = data_explore |>
  select(where(is.numeric), -longitude, -latitude) |>
  variable.names()
```

___

### printing boxplots

```{r warning = F}
map(y_var, print_boxplot)
```

___

### re-creating function to filter some extreme cases

```{r}
print_boxplot_out = function(.y_var_out) {
  y_var = sym(.y_var_out)
  
  data_explore |>
    filter(rooms_per_household < 50, population_per_household < 20) |>
    ggplot(aes(price_category, {{y_var}})) +
    geom_boxplot(aes(fill = price_category), alpha = 0.5, outlier.alpha = 0.25) +
    scale_fill_manual(values = c("indianred3", "springgreen4"))
}

y_var_out = data_explore |>
  select(rooms_per_household, population_per_household) |>
  variable.names()

map(y_var_out, print_boxplot_out)
```

___

### using ``ggscatmat` to create more pairwise plots

```{r warning = F}
data_explore |>
  sample_n(100) |> # speed
  select(price_category, median_income, bedrooms_per_room,
         rooms_per_household, population_per_household) |>
  ggscatmat(color = "price_category", corMethod = "spearman", alpha = 0.25) +
  scale_color_manual(values = c("indianred3", "springgreen4"))
```

___

### exploring categorical variables

```{r}
data_explore |>
  filter(ocean_proximity != "ISLAND") |>
  count(price_category, ocean_proximity) |>
  group_by(price_category) |>
  mutate(percent = n / sum(n) * 100,
         percent = round(percent, 2),
         percent = paste0(percent, "%")) |>
  ggplot(aes(ocean_proximity, n)) +
  geom_col(aes(fill = price_category), position = "dodge") +
  geom_text(aes(label = percent), position = position_dodge2(width = 0.9), vjust = -0.5, size = 3.25) +
  scale_fill_manual(values = c(custom_red, custom_green)) # this step is where i added these as variables
```

___

### creating heatmap

```{r}
data_explore |>
  ggplot(aes(price_category, ocean_proximity)) +
  geom_bin2d() +
  stat_bin2d(geom = "text", aes(label = ..count..)) +
  scale_fill_continuous(type = "viridis")
```

___

### data preparation steps

- handle missing values
- fix or remove outliers
- feature selection
- feature engineering
- feature scaling
- create a validation set

___

### beginning data prep

```{r}
housing_df_new = housing_df |>
  select(longitude, latitude,
         price_category, median_income, ocean_proximity,
         bedrooms_per_room, rooms_per_household, population_per_household)

glimpse(housing_df_new)
```

___

### making new data split

```{r}
set.seed(123)
data_split = initial_split(housing_df_new, prop = 3 / 4, strata = price_category)
train_data = training(data_split)
test_data = testing(data_split)

paste0("training data: ", nrow(train_data), " observations")
paste0("testing data: ", nrow(test_data), " observations")
```

___

### creating preprocessing recipe

```{r}
housing_rec = recipe(price_category ~ ., data = train_data) |>
  update_role(longitude, latitude, new_role = "ID") |>
  # log transforms our skewed data
  step_log(median_income, bedrooms_per_room, rooms_per_household, population_per_household) |>
  # removes any NAs
  step_naomit(everything(), skip = T) |>
  # converts nominal variables to factors
  step_novel(all_nominal(), -all_outcomes()) |>
  # normalizes numeric variables, z-standardization
  step_normalize(all_numeric(), - all_outcomes(),
                 -longitude, -latitude) |>
  # converts `ocean_proximity` to numeric binary
  step_dummy(all_nominal(), -all_outcomes()) |>
  # removes numeric variables with zero variance
  step_zv(all_numeric(), -all_outcomes()) |>
  # removes predictor variables that are highly correlated with other predictor variables
  step_corr(all_predictors(), threshold = 0.7, method = "spearman")

summary(housing_rec)
```

___

### checking out the prepped data

```{r}
prepped_data = housing_rec |>
  prep() |>
  juice()

glimpse(prepped_data)
```

___

### visualising the numeric prepped data

```{r warning = F}
prepped_data |>
  sample_n(100) |> # speed
  select(price_category, median_income, rooms_per_household, population_per_household) |>
  ggscatmat(corMethod = "spearman", alpha = 0.25)
```

___

### cross-validation

```{r}
set.seed(100)
cv_folds = vfold_cv(train_data, v = 5, strata = price_category)
# will come back to this after specifying models
```

___

### specifying models

1. pick a `model type`
2. set the `engine`
3. set the `mode` (regression or classification)

___

### specifying logistic regression model

```{r}
log_spec = logistic_reg() |> # model type
  set_engine(engine = "glm") |> # model engine
  set_mode("classification") # model mode

log_spec
```

___

### specifying random forest model

```{r}
rf_spec = rand_forest() |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("classification")

rf_spec
```

___

### specifying boosted tree (XGBoost) model

```{r}
xgb_spec = boost_tree() |>
  set_engine("xgboost") |>
  set_mode("classification")

xgb_spec
```

___

### specifying k-nearest neighbor model

```{r}
knn_spec = nearest_neighbor(neighbors = 4) |> # note that the number of neighbors can be specified
  set_engine("kknn") |>
  set_mode("classification")

knn_spec
```

___

### specifying neural network model

```{r}
nnet_spec = mlp() |>
  set_mode("classification") |>
  set_engine("keras", verbose = 0)

nnet_spec
```

___

### creating logistic regression workflow

```{r}
log_wflow = workflow() |>
  add_recipe(housing_rec) |>
  add_model(log_spec)

log_wflow
```

___

### creating random forest workflow

```{r}
rf_wflow = workflow() |>
  add_recipe(housing_rec) |>
  add_model(rf_spec)

rf_wflow
```

___

### creating XGBoost workflow

```{r}
xgb_wflow = workflow() |>
  add_recipe(housing_rec) |>
  add_model(xgb_spec)

xgb_wflow
```

___

### creating k-nearest neighbor workflow

```{r}
knn_wflow = workflow() |>
  add_recipe(housing_rec) |>
  add_model(knn_spec)

knn_wflow
```

___

### creating neural network workflow

```{r}
nnet_wflow = workflow() |>
  add_recipe(housing_rec) |>
  add_model(nnet_spec)

nnet_wflow
```

___

### evaluating logistic regression

```{r warning = F}
log_res = log_wflow |>
  fit_resamples(resamples = cv_folds,
                metrics = metric_set(recall, precision, f_meas,
                                     accuracy, kap, roc_auc, sens, spec),
                control = control_resamples(save_pred = T))

log_res
```

___

### getting model coefficients

```{r}
get_model = function(x) {
  pull_workflow_fit(x) |>
    tidy()
}

log_res_2 = log_wflow |>
  fit_resamples(resamples = cv_folds,
                metrics = metric_set(recall, precision, f_meas,
                                     accuracy, kap, roc_auc, sens, spec),
                control = control_resamples(save_pred = T, extract = get_model))

log_res_2$.extracts[[1]][[1]]
```

___

### show all resample coefficients for a single predictor

```{r}
all_coef = map_dfr(log_res_2$.extracts, ~ .x[[1]][[1]])
filter(all_coef, term == "median_income")
```

___

### show average performance over all folds

```{r}
log_res |>
  collect_metrics(summarise = T)
```

___

### collecting predictions and printing confusion matrix

```{r}
log_pred = log_res |>
  collect_predictions()

log_pred |>
  conf_mat(price_category, .pred_class)
```

___

### visualising confusion matrix

```{r}
log_pred |>
  conf_mat(price_category, .pred_class) |>
  autoplot(type = "heatmap")
```

___

### ROC curve

```{r}
log_pred |>
  group_by(id) |> # id = folds
  roc_curve(price_category, .pred_above) |>
  autoplot()
```

___

### probability distributions

```{r}
log_pred |>
  ggplot(aes(.pred_above)) +
  geom_density(aes(fill = price_category), alpha = 0.75) +
  scale_fill_manual(values = c(custom_red, custom_green))
```

___

### collecting random forest metrics

```{r}
rf_res = rf_wflow |>
  fit_resamples(resamples = cv_folds,
                metrics = metric_set(recall, precision, f_meas,
                                     accuracy, kap, roc_auc, sens, spec),
                control = control_resamples(save_pred = T))

rf_res |>
  collect_metrics(summarise = T)
```

___

### collecting XGBoost metrics

```{r warning = F}
xgb_res = xgb_wflow |>
  fit_resamples(resamples = cv_folds,
                metrics = metric_set(recall, precision, f_meas,
                                     accuracy, kap, roc_auc, sens, spec),
                control = control_resamples(save_pred = T))

xgb_res |>
  collect_metrics(summarise = T)
```

___

### collecting k-nearest neighbors metrics

```{r warning = F}
knn_res = knn_wflow |>
  fit_resamples(resamples = cv_folds,
                metrics = metric_set(recall, precision, f_meas,
                                     accuracy, kap, roc_auc, sens, spec),
                control = control_resamples(save_pred = T))

knn_res |>
  collect_metrics(summarise = T)
```

___

### collecting neural network metrics

```{r}
# nnet_res = nnet_wflow |>
#   fit_resamples(resamples = cv_folds,
#                 metrics = metric_set(recall, precision, f_meas,
#                                      accuracy, kap, roc_auc, sens, spec),
#                 control = control_resamples(save_pred = T))
# 
# nnet_res |>
#   collect_metrics(summarise = T)
```

___

### compare models

```{r}
log_metrics = log_res |>
  collect_metrics(summarise = T) |>
  mutate(model = "logistic regression")

rf_metrics = rf_res |>
  collect_metrics(summarise = T) |>
  mutate(model = "random forest")

xgb_metrics = xgb_res |>
  collect_metrics(summarise = T) |>
  mutate(model = "xgboost")

knn_metrics = knn_res |>
  collect_metrics(summarise = T) |>
  mutate(model = "k-nearest neighbors")

# nnet_metrics = nnet_res |>
#   collect_metrics(summarise = T) |>
#   mutate(model = "neural network")

# create data frame with all the model results
model_compare = bind_rows(log_metrics, rf_metrics, xgb_metrics, knn_metrics)
```

___

### change data format & visualise F1 scores

```{r}
model_comp = model_compare |>
  select(model, .metric, mean, std_err) |>
  pivot_wider(names_from = .metric, values_from = c(mean, std_err))

model_comp |>
  arrange(mean_f_meas) |>
  mutate(model = fct_reorder(model, mean_f_meas)) |>
  ggplot(aes(model, mean_f_meas)) +
  geom_col(aes(fill = model), alpha = 0.75) +
  coord_flip() +
  geom_text(aes(label = round(mean_f_meas, 3), y = mean_f_meas + 0.08),
            vjust = 1, size = 3)
```

___

### visualising mean area under curve per model

```{r}
model_comp |>
  arrange(mean_roc_auc) |>
  mutate(model = fct_reorder(model, mean_roc_auc)) |>
  ggplot(aes(model, mean_roc_auc)) +
  geom_col(aes(fill = model), alpha = 0.75) +
  coord_flip() +
  geom_text(aes(label = round(mean_roc_auc, 3),
                y = mean_roc_auc + 0.08), vjust = 1, size = 3)
```

___

### getting best model (based on F1 score)

```{r}
model_comp |>
  slice_max(mean_f_meas)
```

___

### last evaluation on test set

```{r}
last_fit_rf = last_fit(rf_wflow,
                       split = data_split,
                       metrics = metric_set(recall, precision, f_meas,
                                            accuracy, kap, roc_auc, sens, spec))

last_fit_rf |>
  collect_metrics()
```

___

### seeing how metrics compare to training evaluation

```{r}
train_eval = model_comp |>
  slice_max(mean_f_meas) |>
  pivot_longer(!model, names_to = "metric", values_to = "value") |>
  filter(str_detect(metric, "mean")) |>
  mutate(metric = str_remove_all(metric, "mean_"))

test_eval = last_fit_rf |>
  collect_metrics() |>
  transmute(model = "random forest (test)",
            metric = .metric,
            value = .estimate)

bind_rows(train_eval, test_eval) |>
  mutate(model = ifelse(model == "random forest", "validation", "test")) |>
  ggplot(aes(metric, value)) +
  geom_col(aes(fill = model), position = "dodge") +
  geom_text(aes(label = round(value, 3)),
            position = position_dodge2(width = 1), size = 3, hjust = -0.25) +
  coord_flip(ylim = c(0, 1)) +
  scale_fill_manual(values = c("#98B1CF", "#7FB287")) +
  labs(fill = "evaluation set", title = "metrics roughly the same; minimal overfitting") +
  theme(plot.title = element_text(hjust = 0.5))
```

___

### variable importance scores

```{r}
last_fit_rf |>
  pluck(".workflow", 1) |>
  pull_workflow_fit() |>
  vip(num_features = 10)
```

___

### final confusion matrix

```{r}
last_fit_rf |>
  collect_predictions() |>
  conf_mat(price_category, .pred_class) |>
  autoplot(type = "heatmap")
```

___

### final ROC curve

```{r}
last_fit_rf |>
  collect_predictions() |>
  roc_curve(price_category, .pred_above) |>
  autoplot()
```

___

all results indicate that the validation and test set performance metrics are very similar, which suggests the random forest model will perform well when predicting new out-of-sample data.

___

### total script runtime

```{r}
tictoc::toc()
```
































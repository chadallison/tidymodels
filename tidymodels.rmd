---
title: "classification with tidymodels"
author: "chad allison | 12 december 2022"
output: github_document
---

___

### loading required libraries and setting options

```{r message = F, warning = F}
library(tidyverse) # essential functions
library(tidymodels) # essential for tidy modeling
library(visdat) # visualising data class structure
library(skimr) # data skimming
library(GGally) # pairwise plots
library(ggmap) # geographical visualisation
library(ranger) # random forest
library(keras) # neural network

knitr::opts_chunk$set(message = F, warning = F)
options(scipen = 999)
theme_set(theme_minimal())

custom_red = "#FFCFCF"
custom_green = "#B7D1B3"
tictoc::tic()
```

___

### importing data

```{r}
link = "https://raw.githubusercontent.com/kirenz/datasets/master/housing_unclean.csv"
housing_df = read_csv(link, col_types = cols())
rm(link)

head(housing_df)
```

___

### cleaning `housing_median_age` and `median_house_value`

```{r}
housing_df = housing_df |>
  mutate(housing_median_age = str_remove_all(housing_median_age, "[years]"),
         median_house_value = str_remove_all(median_house_value, "[$]"))

housing_df |>
  select(housing_median_age, median_house_value) |>
  head()
```

___

### viewing data structure

```{r}
glimpse(housing_df)
```

___

### visualising data structure

```{r}
vis_dat(housing_df) +
  scale_fill_manual(values = c("#8BB895", "#B494C6"))
```

___

### exploring `ocean_proximity` variable

```{r}
housing_df |>
  count(ocean_proximity,
        sort = T)
```

___

### reformatting variables

```{r}
housing_df = housing_df |>
  mutate(housing_median_age = as.numeric(housing_median_age),
         median_house_value = as.numeric(median_house_value),
         across(where(is.character), as.factor))

vis_dat(housing_df) +
  scale_fill_manual(values = c("#8BB895", "#B494C6"))
```

___

### checking missing data

```{r}
housing_df |>
  vis_miss(sort_miss = T)
```

___

### seeing exactly how many `NA` values we have

```{r}
colSums(is.na(housing_df))
```

___

### creating new variables

```{r}
housing_df = housing_df |>
  mutate(rooms_per_household = round(total_rooms / households, 4),
         bedrooms_per_room = round(total_bedrooms / total_rooms, 4),
         population_per_household = round(population / households, 4))

housing_df |>
  select(rooms_per_household, bedrooms_per_room, population_per_household) |>
  head()
```

___

### creating dependent variable and dropping original numeric variable

```{r}
housing_df = housing_df |>
  mutate(price_category = case_when(median_house_value < 150000 ~ "below",
                                    median_house_value >= 150000 ~ "above"),
         price_category = as.factor(price_category)) |>
  select(-median_house_value)

housing_df |>
  count(price_category) |>
  mutate(prop = round(n / sum(n), 3))
```

___

### data overview with `skimr`

```{r}
skim(housing_df)
```

### data overview with pariwise plots from `GGally`

```{r message = F, warning = F}
housing_df |>
  sample_n(1000) |> # sampling for script run time
  select(housing_median_age, median_income, rooms_per_household,
         ocean_proximity, price_category) |>
  ggpairs()
```

### data splitting

```{r}
set.seed(123)
data_split = initial_split(housing_df, prop = 0.75, strata = price_category)
train_data = training(data_split)
test_data = testing(data_split)

raw_counts = housing_df |>
  count(price_category) |>
  mutate(prop = round(n / sum(n), 4),
         set = "raw")

train_counts = train_data |>
  count(price_category) |>
  mutate(prop = round(n / sum(n), 4),
         set = "train")

test_counts = test_data |>
  count(price_category) |>
  mutate(prop = round(n / sum(n), 4),
         set = "test")

rbind(raw_counts, train_counts, test_counts) |>
  ggplot(aes(price_category, prop)) +
  geom_col(aes(fill = set), position = "dodge", alpha = 0.75) +
  scale_fill_manual(values = c("#828BA8", "#99BF9E", "#B293BD")) +
  theme_classic() +
  labs(title = "`price_category` equally distributed among full, training, and testing data") +
  theme(plot.title = element_text(hjust = 0.5))

rm(raw_counts, train_counts, test_counts)
```

___

### geographical overview

```{r message = F, warning = F}
data_explore = train_data # so we don't alter our data

qmplot(x = longitude, y = latitude, data = data_explore,
       geom = "point", col = price_category, size = population, alpha = 0.25) +
  scale_alpha(guide = "none") +
  scale_color_manual(values = c("indianred3", "lightsteelblue3"))
```

___

### exploring numeric variables

```{r}
data_explore |>
  ggplot(aes(price_category, median_income)) +
  geom_boxplot(aes(fill = price_category),
               outlier.alpha = 0.25, alpha = 0.5) +
  scale_fill_manual(values = c("indianred3", "springgreen4"))
```

### creating function to print boxplot

```{r}
print_boxplot = function(.y_var) {
  y_var = sym(.y_var)
  
  data_explore |>
    ggplot(aes(price_category, {{y_var}})) +
    geom_boxplot(aes(fill = price_category), outlier.alpha = 0.25, alpha = 0.5) +
    scale_fill_manual(values = c("indianred3", "springgreen4"))
}
```

### obtaining numeric y-variables

```{r}
y_var = data_explore |>
  select(where(is.numeric), -longitude, -latitude) |>
  variable.names()
```

### printing boxplots

```{r warning = F}
map(y_var, print_boxplot)
```

### re-creating function to filter some extreme cases

```{r}
print_boxplot_out = function(.y_var_out) {
  y_var = sym(.y_var_out)
  
  data_explore |>
    filter(rooms_per_household < 50, population_per_household < 20) |>
    ggplot(aes(price_category, {{y_var}})) +
    geom_boxplot(aes(fill = price_category), alpha = 0.5, outlier.alpha = 0.25) +
    scale_fill_manual(values = c("indianred3", "springgreen4"))
}

y_var_out = data_explore |>
  select(rooms_per_household, population_per_household) |>
  variable.names()

map(y_var_out, print_boxplot_out)
```

### using ``ggscatmat` to create more pairwise plots

```{r warning = F}
data_explore |>
  sample_n(1000) |> # speed
  select(price_category, median_income, bedrooms_per_room,
         rooms_per_household, population_per_household) |>
  ggscatmat(color = "price_category", corMethod = "spearman", alpha = 0.25) +
  scale_color_manual(values = c("indianred3", "springgreen4"))
```

### exploring categorical variables

```{r}
data_explore |>
  filter(ocean_proximity != "ISLAND") |>
  count(price_category, ocean_proximity) |>
  group_by(price_category) |>
  mutate(percent = n / sum(n) * 100,
         percent = round(percent, 2),
         percent = paste0(percent, "%")) |>
  ggplot(aes(ocean_proximity, n)) +
  geom_col(aes(fill = price_category), position = "dodge") +
  geom_text(aes(label = percent), position = position_dodge2(width = 0.9), vjust = -0.5, size = 3.25) +
  scale_fill_manual(values = c(custom_red, custom_green)) # this step is where i added these as variables
```

### creating heatmap

```{r}
data_explore |>
  ggplot(aes(price_category, ocean_proximity)) +
  geom_bin2d() +
  stat_bin2d(geom = "text", aes(label = ..count..)) +
  scale_fill_continuous(type = "viridis")
```

### data preparation steps

- handle missing values
- fix or remove outliers
- feature selection
- feature engineering
- feature scaling
- create a validation set

### beginning data prep

```{r}
housing_df_new = housing_df |>
  select(longitude, latitude,
         price_category, median_income, ocean_proximity,
         bedrooms_per_room, rooms_per_household, population_per_household)

glimpse(housing_df_new)
```

### making new data split

```{r}
set.seed(123)
data_split = initial_split(housing_df_new, prop = 3 / 4, strata = price_category)
train_data = training(data_split)
test_data = testing(data_split)

paste0("training data: ", nrow(train_data), " observations")
paste0("testing data: ", nrow(test_data), " observations")
```

### creating preprocessing recipe

```{r}
housing_rec = recipe(price_category ~ ., data = train_data) |>
  update_role(longitude, latitude, new_role = "ID") |>
  # log transforms our skewed data
  step_log(median_income, bedrooms_per_room, rooms_per_household, population_per_household) |>
  # removes any NAs
  step_naomit(everything(), skip = T) |>
  # converts nominal variables to factors
  step_novel(all_nominal(), -all_outcomes()) |>
  # normalizes numeric variables, z-standardization
  step_normalize(all_numeric(), - all_outcomes(),
                 -longitude, -latitude) |>
  # converts `ocean_proximity` to numeric binary
  step_dummy(all_nominal(), -all_outcomes()) |>
  # removes numeric variables with zero variance
  step_zv(all_numeric(), -all_outcomes()) |>
  # removes predictor variables that are highly correlated with other predictor variables
  step_corr(all_predictors(), threshold = 0.7, method = "spearman")

summary(housing_rec)
```

### checking out the prepped data

```{r}
prepped_data = housing_rec |>
  prep() |>
  juice()

glimpse(prepped_data)
```

### visualising the numeric prepped data

```{r warning = F}
prepped_data |>
  sample_n(1000) |> # speed
  select(price_category, median_income, rooms_per_household, population_per_household) |>
  ggscatmat(corMethod = "spearman", alpha = 0.25)
```

### cross-validation

```{r}
set.seed(100)
cv_folds = vfold_cv(train_data, v = 5, strata = price_category)
# will come back to this after specifying models
```

### specifying models

1. pick a `model type`
2. set the `engine`
3. set the `mode` (regression or classification)

### specifying logistic regression model

```{r}
log_spec = logistic_reg() |> # model type
  set_engine(engine = "glm") |> # model engine
  set_mode("classification") # model mode

log_spec
```

### specifying random forest model

```{r}
rf_spec = rand_forest() |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("classification")

rf_spec
```

### specifying boosted tree (XGBoost) model

```{r}
xgb_spec = boost_tree() |>
  set_engine("xgboost") |>
  set_mode("classification")

xgb_spec
```

### specifying k-nearest neighbor model

```{r}
knn_spec = nearest_neighbor(neighbors = 4) |> # note that the number of neighbors can be specified
  set_engine("kknn") |>
  set_mode("classification")

knn_spec
```

### specifying neural network model

```{r}
nnet_spec = mlp() |>
  set_mode("classification") |>
  set_engine("keras", verbose = 0)

nnet_spec
```

### creating logistic regression workflow

```{r}
log_wflow = workflow() |>
  add_recipe(housing_rec) |>
  add_model(log_spec)

log_wflow
```

### creating random forest workflow

```{r}
rf_wflow = workflow() |>
  add_recipe(housing_rec) |>
  add_model(rf_spec)

rf_wflow
```

### creating XGBoost workflow

```{r}
xgb_wflow = workflow() |>
  add_recipe(housing_rec) |>
  add_model(xgb_spec)

xgb_wflow
```

### creating k-nearest neighbor workflow

```{r}
knn_wflow = workflow() |>
  add_recipe(housing_rec) |>
  add_model(knn_spec)

knn_wflow
```

### creating neural network workflow

```{r}
nnet_wflow = workflow() |>
  add_recipe(housing_rec) |>
  add_model(nnet_spec)

nnet_wflow
```

### evaluating logistic regression

```{r warning = F}
log_res = log_wflow |>
  fit_resamples(resamples = cv_folds,
                metrics = metric_set(recall, precision, f_meas,
                                     accuracy, kap, roc_auc, sens, spec),
                control = control_resamples(save_pred = T))

log_res
```




















```{r}
tictoc::toc()
```































